# Training Models #

## Regularization (to prevent overfitting) ##
- use a simpler model
- constrain the weights
- stop early

## Ridge Regression ##
- add sum of squares of weights to the cost function
- this penalizes extremely high weights
- hyperparameter is a factor detemining amount of regularization
- a = 0 is no regularization
- a = infinity, forces all weights to zero, maing the output a constant

## Lasso Regression ##
- LASSO = Least Absolute Shrinkage and Selection Operator
- uses L1 norm instead of L2 norm (abolute value instead of squares)
- tends to shrink unimportant weight to 0

## In Practice: ##

### Elastic Net ###
- weighted average of ridge and LASSO
- early stopping
    - genearlizing worse and worse (overfitting) after a certian point
### Logistic Regression ###

- estimates a probability that a point is in the class
- classifies it that way if this probability is at least .5
- probability is produced by passing weighted sum to the loistic function
- logistic function: $\sigma(t) = 1/(1 + e^-t)$

### Training Logistic Regressoin ###
- log loss cost function
- no equivalent to normal equation, but we can use gradient descent

### Softman Regression ###
- for multiclass classification
- find a weighted sum for each class
- estimate probabilities using softmax function
- cost function: cross entropy (distance between distributions)
    - equivalent to logistic regressino cost function where there are 2 classes
- softmax function: $\sigma(x_k) = exp(x_k)/\sum\exp(x_j)$

## Main Points ##
- linear and polynomial regression
- gradient descent
- bias/variance tradeoff
- regularization (including early stopping)
- logistic regression for classificaiton
