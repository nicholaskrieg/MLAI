# Linear Regression #

- let Y be a function of $x_1$ and $x_2$
    - in graphical representation, you would be producing a plane, not just a line
- linear equations:
    - $y = m*x + b$
    - $\hat{y} = \Theta_1 + \Theta_1*x_1$
    - $\hat{y} = \Theta_0 + \Theta_1*x_1 + ... + \Theta_n*x_n$
- linear equation in vector form
    - $\hat{y} = h_0*x = \Theta^T * x = \sum \Theta_i * x_i$
        - where $x$ is a vector of numbers
        - $\Theta^T * x$ = theta transponsed times $x$
- we're minimizing Mean Squared Error (MSE)
    - taking average of squared error and minimizing that

## Normal Equation: ##
- a closed-form solution exists
- expensive when there are many features
- such solution dont exist for more ocmplex models (logistic regression)

## Gradient Discent: ##
- in this context, gradient means derivative
- looking at slope of line to find the goal of gradient descent
    - looking for point at bottom, zero gradient 
    - from starting point, move in direction of gradient
        - when gradient is positive and large, take a big step in the left direction
        - when gradient is negative and large, take a big step in the right direction
- we can do this for three dimensional spaces as well
- difference between local minima and global minima

    ### Batch gardient Descent: ###
    - find gradient for entire training set
        - sum of gradients for individual data points
    - slow for large date sets

    ### Stochastic gradient descent: ###
    - adjust for one data point at a time in a random order
    - dont have to fit entire training set in memory at once
    - bounces around some due to randomness

    ### Mini-Batch Gradient Descent: ###
    - use a random mini-batch of data points on each step
    - less random than stochastic but doesnt use as much memory

    ### Under and Over Fitting: ###
    - using a higher degree polynomial does not accurately represent of the underlying model
        - trying to exaclty memorize the data
        - won't generalize well to new data 

    ### Biased/Variance Tradeoff: ###
    - sources of error:
        - bias
            - if the model uses a first degree polynomial, model is biased to lines
            - if the model uses second degree polynomials, the model is a little less biased
        - variance
        - irreducible error