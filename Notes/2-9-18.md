# Class Notes 2 - 9 - 2017 #
## Monte Carlo Search Methods ##
### K - Armed Bandit ###
- There is a difference between exploration and explotation 
    - Exploration term: explore/pulls
        - Explores is a constant you set
    - Explotation term: wins/pulls 
        - How many times you won divided by how many times you tried
- Note upper confidence bound
    - Either a move/strategy that has high mean or very wide confidence interval (CI)

### Monte Carlo Tree Search ###
- Build search tree as data structure
    - Different form Minimax search
- Tree search process:
    1. Create a root node
    2. For thousands of times:
        - Play a simulated game from the current state
            1. Start at the root
            2. Use upper confidence bounds formula
                - Try which ever move looks most appealing
            3. Move to next child until you "fall off the bottom of the tree"
                - When you fall off the bottom, add a new leaf (expand the tree)
            4. Play random moves until the end of the game
        - Go back through all nodes you visited in the simulation and update search tree
    3. Play best move from root
- Each node knows, for each one of its children, its number of pulls and number of wins
- Note: When the simulation is finished, choose the move that has the most wins
    - Choosing the move with the highest wins/simulations ratio or the move with the highest upper confidence bound is not as good

